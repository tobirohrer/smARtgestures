{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers.python.layers.layers import convolution\n",
    "from tensorflow.python.ops.nn_impl import relu_layer\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import dataImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 20\n",
    "batch_size = 32\n",
    "neurons_fc = 32\n",
    "conv_filter_size = 5\n",
    "feature_maps_layer1 = 16\n",
    "feature_maps_layer2 = 32\n",
    "output_vector_size = 4\n",
    "learn_rate = 1e-4\n",
    "eval_data_amount = 600\n",
    "\n",
    "gesture_data_dir = '../../gestureData' \n",
    "\n",
    "#Variables\n",
    "#source http://cs231n.github.io/neural-networks-2/\n",
    "bias_init_factor = 0.1\n",
    "#random_initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32\n",
    "random_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)\n",
    "#random_initializer = tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None,40,40], name='input_data')\n",
    "    \n",
    "# First \"Layer Stack\"\n",
    "with tf.name_scope('conv_layer_1'):\n",
    "    filters_layer1 = tf.get_variable(\"filter1_variables\", \n",
    "                                     [conv_filter_size, conv_filter_size, 1, feature_maps_layer1], \n",
    "                                     initializer = random_initializer)\n",
    "    bias_layer1 = tf.get_variable(\"bias1_variables\",\n",
    "                                  [feature_maps_layer1],\n",
    "                                  initializer = tf.constant_initializer(bias_init_factor))\n",
    "    reshaped_input_data = tf.reshape(input_data, [-1, 40, 40, 1])\n",
    "    convolution_layer1 = conv2d(reshaped_input_data, filters_layer1) + bias_layer1\n",
    "\n",
    "with tf.name_scope('pooling_layer_1'):\n",
    "    pooling_layer1 = max_pool_2x2(convolution_layer1)\n",
    "    \n",
    "with tf.name_scope('ReLU_layer_1'):\n",
    "    relu_layer1 = tf.nn.relu(pooling_layer1)\n",
    "\n",
    "# Second \"Layer Stack\"\n",
    "with tf.name_scope('conv_layer_2'):\n",
    "    filters_layer2 = tf.get_variable(\"filter2_variables\",\n",
    "                                     [conv_filter_size, conv_filter_size, feature_maps_layer1, feature_maps_layer2],\n",
    "                                     initializer = random_initializer)\n",
    "    bias_layer2 = tf.get_variable(\"bias2_variables\",\n",
    "                                  [feature_maps_layer2],\n",
    "                                  initializer = tf.constant_initializer(bias_init_factor))\n",
    "    convolution_layer2 = conv2d(relu_layer1, filters_layer2) + bias_layer2\n",
    "\n",
    "with tf.name_scope('pooling_layer_2'):\n",
    "    pooling_layer2 = max_pool_2x2(convolution_layer2)    \n",
    "\n",
    "with tf.name_scope('ReLU_layer_2'):\n",
    "    relu_layer2 = tf.nn.relu(pooling_layer2)\n",
    "    \n",
    "\n",
    "    \n",
    "# Start Fully connected layers\n",
    "with tf.name_scope('fc_layer_1'):\n",
    "    weights_fc1 = tf.get_variable(\"fc1_variables\",[10 * 10 * feature_maps_layer2, neurons_fc],\n",
    "                                  initializer = random_initializer)\n",
    "    \n",
    "    bias_fc1 = tf.get_variable(\"bias_fc1_variables\",\n",
    "                               [neurons_fc],\n",
    "                               initializer = tf.constant_initializer(bias_init_factor))\n",
    "\n",
    "    pool_layer2_flat = tf.reshape(relu_layer2, [-1, 10*10*feature_maps_layer2])\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool_layer2_flat, weights_fc1) + bias_fc1)\n",
    "    \n",
    "with tf.name_scope('dropout_layer'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1_dropout = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope('fc_layer_2'):\n",
    "    weights_fc2 = tf.get_variable(\"fc2_variables\",[neurons_fc, output_vector_size], initializer = random_initializer)\n",
    "    bias_fc2 = tf.get_variable(\"bias_fc2_variables\",\n",
    "                               [output_vector_size],\n",
    "                               initializer = tf.constant_initializer(bias_init_factor))\n",
    "\n",
    "    output_class_probabilities = tf.matmul(fc1_dropout, weights_fc2) + bias_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_label = tf.placeholder(tf.float32, shape=[None,4], name='desired_label')\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=desired_label, logits=output_class_probabilities))\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "    train_step = optimizer.minimize(error)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(output_class_probabilities, 1), tf.argmax(desired_label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('session'):\n",
    "    session = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/trainData2.json\"), True, True)\n",
    "shuffle(train_data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "#ensure random eval data is equally distributed through all gesture types!\n",
    "eval_data_class_counts = {0:0,1:0,2:0,3:0}\n",
    "while len(eval_data) < eval_data_amount:\n",
    "    element = train_data.pop()\n",
    "    element_class_id = np.array(element[1]).nonzero()[0][0]\n",
    "    if eval_data_class_counts[element_class_id]<int(eval_data_amount/4):\n",
    "        eval_data_class_counts[element_class_id] = eval_data_class_counts[element_class_id] + 1\n",
    "        eval_data.append(element)\n",
    "    else:\n",
    "        train_data.insert(0,element)\n",
    "    \n",
    "eval_input_batch = []\n",
    "eval_label_batch = []\n",
    "for j in range(eval_data_amount):\n",
    "    eval_input_batch.append(eval_data[j][0])\n",
    "    eval_label_batch.append(eval_data[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data =[]\n",
    "test_data = dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/testData.json\"), True, False)\n",
    "test_data_count = len(test_data)\n",
    "shuffle(test_data)\n",
    "\n",
    "print(test_data_count)\n",
    "\n",
    "test_input_batch = []\n",
    "test_label_batch = []\n",
    "for j in range(test_data_count):\n",
    "    test_input_batch.append(test_data[j][0])\n",
    "    test_label_batch.append(test_data[j][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fremdgesten_data = []\n",
    "fremdgesten_data = dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person1.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person2.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person3.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person4.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete2D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person5.json\"), True, False)\n",
    "fremdgesten_data_count = len(fremdgesten_data)\n",
    "shuffle(fremdgesten_data)\n",
    "print(\"its \" + repr(fremdgesten_data_count) + \" fremdgesten\")\n",
    "\n",
    "fremdgesten_input_batch = []\n",
    "fremdgesten_label_batch = []\n",
    "for j in range(fremdgesten_data_count):\n",
    "    fremdgesten_input_batch.append(fremdgesten_data[j][0])\n",
    "    fremdgesten_label_batch.append(fremdgesten_data[j][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('../../tensorboard/tmp2', graph=tf.get_default_graph())\n",
    "\n",
    "# create a summary for our error and accuracy\n",
    "acuracy_op_eval = tf.summary.scalar(\"accuracy_eval\", accuracy)\n",
    "acuracy_op_test = tf.summary.scalar(\"accuracy_test\", accuracy)\n",
    "acuracy_op_train = tf.summary.scalar(\"accuracy_train\", accuracy)\n",
    "error_op_eval = tf.summary.scalar(\"error_eval\", error)\n",
    "error_op_test = tf.summary.scalar(\"error_test\", error)\n",
    "error_op_train = tf.summary.scalar(\"error_train\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = int(len(train_data)/batch_size)\n",
    "print(\"training on \" + repr(len(train_data)) + \" gestures\")\n",
    "print(\"thats \" + repr(iterations) + \" per epoch\")\n",
    "\n",
    "train_data_count = len(train_data)\n",
    "\n",
    "with tf.name_scope('training...'):\n",
    "    for e in range(epoch_count):\n",
    "        print(\"starting epoch \" + repr(e))\n",
    "        shuffle(train_data)\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            input_batch = []\n",
    "            label_batch = []\n",
    "            for z in range(batch_size):\n",
    "                input_batch.append(train_data[i*batch_size+z][0])\n",
    "                label_batch.append(train_data[i*batch_size+z][1])\n",
    "            \n",
    "            session.run(train_step,feed_dict={input_data: input_batch, desired_label: np.matrix(label_batch), keep_prob: 0.5})\n",
    "            if(i%500 == 0):\n",
    "                print(session.run(accuracy,feed_dict={input_data: eval_input_batch, desired_label: np.matrix(eval_label_batch), keep_prob:1.0}))\n",
    "            \n",
    "            if(i%100 == 0):\n",
    "                #tensorboard logging!\n",
    "                train_data_batch = []\n",
    "                for j in range(eval_data_amount):\n",
    "                    train_data_batch.append(train_data[j])\n",
    "                \n",
    "                train_input_batch = []\n",
    "                train_label_batch = []\n",
    "                for j in range(eval_data_amount):\n",
    "                    train_input_batch.append(train_data_batch[j][0])\n",
    "                    train_label_batch.append(train_data_batch[j][1])\n",
    "\n",
    "                \n",
    "                error_summary_eval = session.run(error_op_eval,feed_dict={input_data: eval_input_batch, desired_label: eval_label_batch, keep_prob:1.0})\n",
    "                error_summary_test = session.run(error_op_test,feed_dict={input_data: test_input_batch, desired_label: test_label_batch, keep_prob:1.0})\n",
    "                error_summary_train = session.run(error_op_train,feed_dict={input_data: train_input_batch, desired_label: train_label_batch, keep_prob:1.0})\n",
    "                \n",
    "                acuracy_summary_eval = session.run(acuracy_op_eval,feed_dict={input_data: eval_input_batch, desired_label: eval_label_batch, keep_prob:1.0})\n",
    "                acuracy_summary_test = session.run(acuracy_op_test,feed_dict={input_data: test_input_batch, desired_label: test_label_batch, keep_prob:1.0})\n",
    "                acuracy_summary_train = session.run(acuracy_op_train,feed_dict={input_data: train_input_batch, desired_label: train_label_batch, keep_prob:1.0})\n",
    "                \n",
    "                writer.add_summary(error_summary_eval, i+((e)*iterations))\n",
    "                writer.add_summary(acuracy_summary_eval, i+((e)*iterations))\n",
    "                writer.add_summary(error_summary_test, i+((e)*iterations))\n",
    "                writer.add_summary(acuracy_summary_test, i+((e)*iterations))\n",
    "                writer.add_summary(error_summary_train, i+((e)*iterations))\n",
    "                writer.add_summary(acuracy_summary_train, i+((e)*iterations))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy on testdata:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: test_input_batch, desired_label: np.matrix(test_label_batch), keep_prob:1.0}))\n",
    "print(\"accuracy on evaldata:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: eval_input_batch, desired_label: np.matrix(eval_label_batch), keep_prob:1.0}))\n",
    "print(\"accuracy on fremddata:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: fremdgesten_input_batch, desired_label: np.matrix(fremdgesten_label_batch), keep_prob:1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define which gesturedata should be displayed\n",
    "show_id = 8\n",
    "\n",
    "visualisation_data = []\n",
    "visualisation_data.append(eval_data[show_id][0])\n",
    "\n",
    "conv_layer1 = session.run(convolution_layer1, {input_data: visualisation_data})\n",
    "pool_layer1 = session.run(pooling_layer1, {input_data: visualisation_data})\n",
    "transposed_conv_layer1 = tf.transpose(conv_layer1, [3, 1, 2, 0])\n",
    "transposed_pooling_layer1 = tf.transpose(pool_layer1, [3, 1, 2, 0])\n",
    "transposed_filters_layer1 = tf.transpose(filters_layer1, [3, 0, 1, 2])\n",
    "\n",
    "print(transposed_filters_layer1.shape)\n",
    "\n",
    "#its now an \"array\" of 3D cubes\n",
    "for i in range(feature_maps_layer1):\n",
    "    print(i)\n",
    "    print('input')\n",
    "    plt.matshow(eval_data[show_id][0])\n",
    "    plt.show()\n",
    "    \n",
    "    print('feature map ' + repr(i))\n",
    "    single_feature_map = transposed_conv_layer1[i]\n",
    "    #get rid of nonsense last dimension\n",
    "    conv_3d = tf.reshape(single_feature_map, (40,40))\n",
    "    plt.matshow(session.run(conv_3d))\n",
    "    #print(session.run(conv_3d))\n",
    "    plt.show()\n",
    "    \n",
    "    print('filter ' + repr(i))\n",
    "    single_filter = transposed_filters_layer1[i]\n",
    "    #get rid of nonsense last dimension\n",
    "    filter_2d = tf.reshape(single_filter, (conv_filter_size,conv_filter_size))\n",
    "    plt.matshow(session.run(filter_2d))\n",
    "    #print(session.run(filter_2d))\n",
    "    plt.show()\n",
    "    \n",
    "    print('pool ' + repr(i))\n",
    "    pooled_feature_map = transposed_pooling_layer1[i]\n",
    "    #get rid of nonsense last dimension\n",
    "    pooled_feature_map = tf.reshape(pooled_feature_map, (20,20))\n",
    "    plt.matshow(session.run(pooled_feature_map))\n",
    "    #print(session.run(conv_3d))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#display all convolution filters of first convolution layer\n",
    "f, axarr = plt.subplots(2,8,figsize=(15, 15))\n",
    "im_id = 0\n",
    "f = 0\n",
    "for i in range(2):\n",
    "    for j in range(8):\n",
    "        try:\n",
    "            axarr[i,j].imshow(session.run(tf.reshape(transposed_filters_layer1[f], (conv_filter_size,conv_filter_size))))\n",
    "            axarr[i,j].axis('off')\n",
    "            im_id = im_id+1\n",
    "            f +=1\n",
    "        except:\n",
    "            continue\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newKernel",
   "language": "python",
   "name": "newkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
