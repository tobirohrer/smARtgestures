{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers.python.layers.layers import convolution\n",
    "from tensorflow.python.ops.nn_impl import relu_layer\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import dataImporter\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 10\n",
    "batch_size = 32\n",
    "neurons_fc = 128\n",
    "conv_filter_size = 5\n",
    "feature_maps_layer1 = 16\n",
    "feature_maps_layer2 = 32\n",
    "output_vector_size = 8\n",
    "learn_rate = 1e-3\n",
    "eval_data_amount = 40\n",
    "\n",
    "gesture_data_dir = '../../gestureData' \n",
    "\n",
    "#Variables\n",
    "#source http://cs231n.github.io/neural-networks-2/\n",
    "bias_init_factor = 0.1\n",
    "#random_initializer = tf.contrib.layers.xavier_initializer_conv2d(uniform=True, seed=None, dtype=tf.float32\n",
    "random_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)\n",
    "#random_initializer = tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    input_data = tf.placeholder(tf.float32, shape=[None,40,40,40], name='input_data')\n",
    "    \n",
    "\n",
    "# First \"Layer Stack\"\n",
    "with tf.name_scope('conv_layer_1'):\n",
    "    filters_layer1 = tf.get_variable(\"filter1_variables\",\n",
    "                                     [conv_filter_size, \n",
    "                                      conv_filter_size, conv_filter_size,\n",
    "                                      1,\n",
    "                                      feature_maps_layer1],\n",
    "                                     initializer = random_initializer)\n",
    "    bias_layer1 = tf.get_variable(\"bias1_variables\",\n",
    "                                  [feature_maps_layer1],\n",
    "                                  initializer = tf.constant_initializer(bias_init_factor))\n",
    "    reshapted_input_data = tf.reshape(input_data, [-1, 40, 40, 40, 1])\n",
    "    convolution_layer1 = conv3d(reshapted_input_data, filters_layer1) + bias_layer1\n",
    "\n",
    "with tf.name_scope('pooling_layer_1'):\n",
    "    pooling_layer1 = max_pool_2x2(convolution_layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope('ReLU_layer_1'):\n",
    "    relu_layer1 = tf.nn.relu(pooling_layer1)\n",
    "    \n",
    "\n",
    "    \n",
    "# Second \"Layer Stack\"\n",
    "with tf.name_scope('conv_layer_2'):\n",
    "    filters_layer2 = tf.get_variable(\"filter2_variables\",\n",
    "                                     [conv_filter_size,\n",
    "                                      conv_filter_size,\n",
    "                                      conv_filter_size,\n",
    "                                      feature_maps_layer1,\n",
    "                                      feature_maps_layer2],\n",
    "                                     initializer = random_initializer)\n",
    "    bias_layer2 = tf.get_variable(\"bias2_variables\",\n",
    "                                  [feature_maps_layer2],\n",
    "                                  initializer = tf.constant_initializer(bias_init_factor))    \n",
    "    convolution_layer2 = conv3d(relu_layer1, filters_layer2) + bias_layer2\n",
    "    \n",
    "with tf.name_scope('pooling_layer_2'):\n",
    "    pooling_layer2 = max_pool_2x2(convolution_layer2)\n",
    "\n",
    "with tf.name_scope('ReLU_layer_2'):\n",
    "    relu_layer2 = tf.nn.relu(pooling_layer2)\n",
    "\n",
    "    \n",
    "# Start Fully connected layers    \n",
    "with tf.name_scope('fc_layer_1'):\n",
    "    weights_fc1 = tf.get_variable(\"fc1_variables\",\n",
    "                            [10 * 10 * 10 * feature_maps_layer2, neurons_fc],\n",
    "                            initializer = random_initializer)\n",
    "    bias_fc1 = tf.get_variable(\"bias_fc1_variables\",\n",
    "                               [neurons_fc],\n",
    "                               initializer = tf.constant_initializer(bias_init_factor))\n",
    "\n",
    "    pool_layer2_flat = tf.reshape(relu_layer2, [-1, 10 * 10 * 10*feature_maps_layer2])\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool_layer2_flat, weights_fc1) + bias_fc1)\n",
    "    \n",
    "with tf.name_scope('dropout_layer'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1_dropout = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope('fc_layer_2'):\n",
    "    weights_fc2 = tf.get_variable(\"fc2_variables\",\n",
    "                                  [neurons_fc, output_vector_size],\n",
    "                                  initializer = random_initializer)\n",
    "    bias_fc2 = tf.get_variable(\"bias_fc2_variables\",\n",
    "                               [output_vector_size],\n",
    "                               initializer = tf.constant_initializer(bias_init_factor))\n",
    "    \n",
    "    output_class_probabilities = tf.matmul(fc1_dropout, weights_fc2) + bias_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_label = tf.placeholder(tf.float32, shape=[None,8], name='desired_label')\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=desired_label, logits=output_class_probabilities))\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "    train_step = optimizer.minimize(error)\n",
    "        \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(output_class_probabilities, 1), tf.argmax(desired_label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/trainData2.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/trainData.json\"), True, True)\n",
    "train_data = train_data + dataImporter.get_discrete3D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/trainData2.json\"), True, True)\n",
    "shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "#ensure random eval data is equally distributed through all gesture types!\n",
    "eval_data_class_counts = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0}\n",
    "while len(eval_data) < eval_data_amount:\n",
    "    element = train_data.pop()\n",
    "    element_class_id = np.array(element[1]).nonzero()[0][0]\n",
    "    if eval_data_class_counts[element_class_id]<int(eval_data_amount/8):\n",
    "        eval_data_class_counts[element_class_id] = eval_data_class_counts[element_class_id] + 1\n",
    "        eval_data.append(element)\n",
    "    else:\n",
    "        train_data.insert(0,element)\n",
    "    \n",
    "decompressed_batch = dataImporter.decompress_3D(eval_data)\n",
    "eval_input_batch = []\n",
    "eval_label_batch = []\n",
    "for j in range(eval_data_amount):\n",
    "    eval_input_batch.append(decompressed_batch[j][0])\n",
    "    eval_label_batch.append(decompressed_batch[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_data = dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/line/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/circle/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/v/testData.json\"), True, False)\n",
    "test_data = test_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/wave/testData.json\"), True, False)\n",
    "test_data_count = len(test_data)\n",
    "shuffle(test_data)\n",
    "print(\"its \" + repr(test_data_count) + \" testdata\")\n",
    "\n",
    "decompressed_batch = dataImporter.decompress_3D(test_data)\n",
    "test_input_batch = []\n",
    "test_label_batch = []\n",
    "for j in range(test_data_count):\n",
    "    test_input_batch.append(decompressed_batch[j][0])\n",
    "    test_label_batch.append(decompressed_batch[j][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fremdgesten_data = []\n",
    "fremdgesten_data = dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person1.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person2.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person3.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person4.json\"), True, False)\n",
    "fremdgesten_data = fremdgesten_data + dataImporter.get_discrete4D_data(dataImporter.read_data_from_file(gesture_data_dir + \"/fremdGestendaten/person5.json\"), True, False)\n",
    "fremdgesten_data_count = len(fremdgesten_data)\n",
    "shuffle(fremdgesten_data)\n",
    "print(\"its \" + repr(fremdgesten_data_count) + \" fremdgesten\")\n",
    "\n",
    "decompressed_batch = dataImporter.decompress_3D(fremdgesten_data)\n",
    "fremdgesten_input_batch = []\n",
    "fremdgesten_label_batch = []\n",
    "for j in range(fremdgesten_data_count):\n",
    "    fremdgesten_input_batch.append(decompressed_batch[j][0])\n",
    "    fremdgesten_label_batch.append(decompressed_batch[j][1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('../../tensorboard/tmp', graph=tf.get_default_graph())\n",
    "\n",
    "acuracy_op_eval = tf.summary.scalar(\"accuracy_eval\", accuracy)\n",
    "acuracy_op_test = tf.summary.scalar(\"accuracy_test\", accuracy)\n",
    "acuracy_op_train = tf.summary.scalar(\"accuracy_train\", accuracy)\n",
    "error_op_eval = tf.summary.scalar(\"error_eval\", error)\n",
    "error_op_test = tf.summary.scalar(\"error_test\", error)\n",
    "error_op_train = tf.summary.scalar(\"error_train\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('session'):\n",
    "    session = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restore if there is already a trained model..\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, \"./storedSessions/4D/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = int(len(train_data)/batch_size)\n",
    "print(\"training on \" + repr(len(train_data)) + \" gestures\")\n",
    "print(\"evaluating on \" + repr(len(eval_data)) + \" gestures\")\n",
    "print(\"thats \" + repr(iterations) + \" iterations per epoch\")\n",
    "print(\"...and its \" + repr(epoch_count) + \" epochs, so lean back and wait!\")\n",
    "\n",
    "with tf.name_scope('training...'):\n",
    "    for e in range(epoch_count):\n",
    "        print(\"\")\n",
    "        print(\"starting epoch \" + repr(e))\n",
    "        shuffle(train_data)\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            compressed_batch = []\n",
    "            for j in range(batch_size):\n",
    "                compressed_batch.append(train_data[i*batch_size+j])\n",
    "            decompressed_batch = dataImporter.decompress_3D(compressed_batch)\n",
    "            input_batch = []\n",
    "            label_batch = []\n",
    "            for j in range(batch_size):\n",
    "                input_batch.append(decompressed_batch[j][0])\n",
    "                label_batch.append(decompressed_batch[j][1])\n",
    "\n",
    "            session.run(train_step,feed_dict={input_data: input_batch, desired_label: label_batch, keep_prob: 0.5})\n",
    "            \n",
    "            #monitor training process:\n",
    "            if(i%100 == 0):\n",
    "                print(session.run(accuracy, feed_dict={input_data: eval_input_batch, desired_label: eval_label_batch, keep_prob: 1.0}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy on testdata:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: test_input_batch, desired_label: np.matrix(test_label_batch), keep_prob:1.0}))\n",
    "print(\"accuracy on evaldata:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: eval_input_batch, desired_label: np.matrix(eval_label_batch), keep_prob:1.0}))\n",
    "print(\"accuracy on fremdgesten:\")\n",
    "print(session.run(accuracy,feed_dict={input_data: fremdgesten_input_batch, desired_label: np.matrix(fremdgesten_label_batch), keep_prob:1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation (Draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "xs,ys,zs = np.nonzero(eval_data[1][0])\n",
    "ax.scatter(xs,zs,ys)\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Z Label')\n",
    "ax.set_zlabel('Y Label')\n",
    "plt.show()\n",
    "\n",
    "conv_layer1 = session.run(convolution_layer1, {x: np.matrix(eval_data[1][0].flatten())})\n",
    "transposed_conv_layer1 = tf.transpose(conv_layer1, [4, 1, 2, 3,0])\n",
    "\n",
    "#its now an \"array\" of 3D cubes\n",
    "for i in range(transposed_conv_layer1.shape[0]):\n",
    "    single_feature_map = transposed_conv_layer1[i]\n",
    "    print(i)\n",
    "    #get rid of nonsense last dimension\n",
    "    conv_3d = tf.reshape(single_feature_map, (40,40,40))\n",
    "    #plot\n",
    "    fig= plt.figure(i)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x1, y1, z1 = np.nonzero(np.around(session.run(conv_3d)))\n",
    "    ax.scatter(x1,z1,y1)\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Z Label')\n",
    "    ax.set_zlabel('Y Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_filters_layer1 = tf.transpose(filters_layer1, [4, 0, 1, 2, 3])\n",
    "for i in range(transposed_filters_layer1.shape[0]):\n",
    "    single_filter = transposed_filters_layer1[i]\n",
    "    print(i)\n",
    "    #get rid of nonsense last dimension\n",
    "    filter_3d = tf.reshape(single_filter, (conv_filter_size,conv_filter_size,conv_filter_size))\n",
    "    #plot\n",
    "    fig= plt.figure(i)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x1, y1, z1 = np.nonzero(session.run(filter_3d))\n",
    "    c = session.run(filter_3d).flatten()\n",
    "    #dunkel ist mehr und nicht andersrum\n",
    "    c = c*-1\n",
    "    ax.scatter(x1, y1, z1, c=c, cmap=plt.gray())\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Z Label')\n",
    "    ax.set_zlabel('Y Label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transposed_filters_layer1 = tf.transpose(filters_layer1, [4, 0, 1, 2, 3])\n",
    "single_filter = transposed_filters_layer1[0]\n",
    "#get rid of nonsense last dimension\n",
    "filter_3d = tf.reshape(single_filter, (conv_filter_size,conv_filter_size,conv_filter_size))\n",
    "#plot\n",
    "fig= plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x1, y1, z1 = np.nonzero(session.run(filter_3d))\n",
    "c = session.run(filter_3d).flatten()\n",
    "for i in range(len(c)):\n",
    "    if(c[i] < 0.1):\n",
    "        c[i] = 0\n",
    "#dunkel ist mehr und nicht andersrum\n",
    "c = c*-1\n",
    "ax.scatter(x1, z1, y1, c=c, cmap=plt.gray())\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Z Label')\n",
    "ax.set_zlabel('Y Label')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
